Machine Learning Project Enron POI

1:  The goal of this project is to use machine learning algorithms, feature selection and evaluation to find potential persons of interest(POI) in the data. The data itself is all numerical, but split into two groups, financial and email data for individuals at Enron. The financial data is the salary, bonuses, etc. of each person and the email data is the counts of emails to and from POI as well as totals. There is also a binary feature for identifying known POI, 1 for POI, 0 for not. Many features had no values, lots of zeros. The POI feature as labels can be used along with machine learning algorithms to predict POI in data given.
There was one outlier in the data set, total, which was removed, and several mistakes that were addressed. Travel_agency_in_the_park and the only person with all NaN’s were also removed. The errors were incorrectly signed entries, as well as entries in the wrong place. Both found and fixed before creating the features and labels. Total data points: 146, total POI: 18.

2:  The original feature selection process didn’t use pipelines or cross validation, but basic decision trees, PCA, and selectkbest/selectprecentile. MinMaxScaling was performed to allow features like from_poi_to_this_person and certain financial features to carry the same weight as others. The features that counted an action, number of emails, couldn’t compare to currency counts. Some were not as substantial as others, like expenses(total 2,514,970) compared to exercised stock options(total 311,764,000). The features added to the data were emails ratio to/from POI to this person. I thought if the emails played a bigger role it would be interesting. They did end up helping my search. Without the new features, GaussianNB had a precision of .412 and a recall of .263. With the new features, the precision was .425 and recall was .320.
The final feature selection was done using PCA in a pipeline with gridsearchCV. But here are my results from earlier attempts. Decision tree classifier showed loan_advances in a steady lead with at least a .2 feature importance along with long_term_incentive,  from_this_person_to_poi, salary, mes_from_poi_ratio and shared_receipt_with_poi all showing up above .1 feature importance at some point. Selectkbest returned expenses: 22.78, total_stock_value: 11.60, loan_advances: 25.1, other: 10.07 and long_term_incentive: 8.96 as the best 5, scores respectively. Other k values like 2 and 10 were looked at but the double digit scores seemed to cut off at about 5. PCA was used in the final scoring with the principal component having a explained_variance_ratio of .3, and the second at .13. 

3:  The algorithm with the best scores in all areas was GaussionNB. The end scores were accuracy: 0.85167, precision: 0.42525, recall: 0.32000 and  F1: 0.36519. GaussianNB was actually one of the very last ones tried. Before I finally read up on pipelines and shufflesplitCV, I tried all the algorithms we were introduced to with multiple parameters without good scores. After I was familiar with them I again exhausted the more complex algorithms and parameters empty handed. Finally, I tried GaussianNB and a blank decision tree classifier and both passed the threshold but GaussianNB was the best. Adaboost gave me great precision with lousy recall, and kneighbors gave me excellent recall but lousy precision. All gave good accuracy, but depending on what the parameters were the scores could be just about anything.

4:  Tuning the parameters of an algorithm depend on the algorithm itself. Each has its own mathematical equations with variables that might need to be adjusted to have the best outcome. Without proper adjusting algorithms could be untrustworthy, inefficient, or just plain wrong. GaussianNB doesn’t have any tunable parameters, but I did plenty of tuning before I choose it. With the decision tree classifiers, you can tune the minimum number of samples required to split a node, affecting the trade off between precision and recall. Or the criterion for the measure of information gain, changing what math is used to come to a decision.

5:  Validation is checking your work. It is where a model is trained on a training data set, then evaluated with a testing data set. The evaluation metric can be the accuracy, precision, recall, etc. Part of validation is splitting the data into training and testing sets, making sure true labeled data points are evenly spread. This can be done by hand or cross validation. Because of the limitations on size of this data set statifiedshuffleplit cross validation was used. This is important because if the entire dataset is modeled and evaluated, the outcome would be biased. Training and testing on the same data leads to untrustworthy results. The mistake many make is using one metric of validation for your checking. I did so at the start of the project until I read that precision and recall were graded. Validation in this project was easy as we were provided tester.py. Along with that I used accuracy, precision and recall. Precision is the measure of true positives when testing our classifier. This is the ratio of how many POI were correctly marked POI with our classifier. Recall is the measure of true negatives when tested. This is the ratio of how many non-POI were correctly marked non-POI with our classifier.

6:  My final scores were accuracy: 0.85167, precision: 0.42525, recall: 0.32000 and  F1: 0.36519. My averages were lower. Accuracy was always high .65 to .95 with precision ranging in the middle, at about .25 to .75. Recall and F1 were almost always lowest, ranging from .05 to .25. A few rare high recall scores were seen with kneighbors and others of up to .95, mostly around .6. The accuracy is a basically how many right over how many total. Precision is related to how many true and false negatives there are and recall is for positives. F1 is a combination of precision and recall, allowing for a better overall view of the data, but not perfect. So our classifier had pretty good accuracy, but was less than half right calling out true positives or true negatives. Not very good, definitely wouldn’t be good to use in court!  
